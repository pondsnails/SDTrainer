# SDTrainer


# 1\. 研究の動機・目的
本仕様書は、意味微分法を用いた学習率最適化手法に関する調査について記述していく。この調査の目的は、自然言語処理タスクにおいて、MASKされた単語の予測を行うためのモデルを構築し、今回提案する学習率最適化手法性能の影響を様々な学習率スケジューラを用いて評価する。
# 2\. 研究の方法と過程
## 2\.1. 入力データについて
入力データはテキストデータであり、この中にはMASKされた単語が含まれている。このMASKされた単語を予測することがタスクの目標である。
## 2\.2. モデルの構築について
2\.2.1 事前学習モデルαの構築 テキストデータを入力として受け取り、MASKされた単語の予測を行うための事前学習モデルαを構築する。この際、既存の自然言語処理モデル（bert-base-cased）を利用する。αの損失関数は交差エントロピー誤差を用いるものとする。選定理由としては、交差エントロピー損失関数は確率分布間の類似性を評価するための一般的な損失関数であり、自然言語処理の多くのタスクで成功を収めてきたからであり、-Σ(Qi\*logPi)で計算される。PとQは離散的な確率分布を表す。
## 2\.3. SDについて
2\.3.1 分類器βを使用した意味微分法による評価 分類器βを用いて、予測単語と正解単語の分類を行い、SDを適用する。これにより、予測単語と正解単語の損失を求めることができる。SDでは評価因子、力量性因子、活動性因子を用い、それぞれpositive-negative、strong-weak、active-passiveを評価基準に使う。
## 2\.4. 学習率スケジューリングの適用
2\.4.1 初期学習率 初期学習率は2.0\*10^-5とする

2\.4.2 スケジューリング関数の選定 選ばれたスケジューリング関数に基づき、学習率A'に遷移する。スケジューリング関数はConstant、Linear、Cosineを用いる

2\.4.3 交差エントロピー誤差による学習率変換 交差エントロピー誤差を用いて、A'を変換し最終的な学習率A''を計算する。計算された学習率A''をモデルの学習に適用する。定式化は以下の通り：

SD :A''=A' ∛Γ  

Γは交差エントロピー誤差から出力された損失とする。

2\.4.4 本研究においてオプティマイザはデフォルトのAdamWを用いて検証することにする。
## 2\.5. 学習の実行について
2\.5.1 損失関数γに基づく学習の実行 選択した損失関数に基づいて、モデルαの学習を実行する。スケジューリングされた学習率を使用し、トレーニングデータに対する損失を最小化するようにモデルのパラメータを調整する。
## 2\.6. 性能評価
三つの学習率スケジューリングを用いてSDの影響を評価、つまり六つのモデルを比較する。
# 3\. 結果と考察①
## 3\.3. 結果
今回SDを用いたモデルはCosineの場合を除き確かに損失が減少したものの、想定よりも減少がわずかなものだった。

![image](https://github.com/pondsnails/SDTrainer/assets/46395085/2fc02d05-2daf-4562-8764-1fcc1a6c4bfb)
![image](https://github.com/pondsnails/SDTrainer/assets/46395085/bd793d33-9a9e-4cb6-8537-136a8278b62c)
![image](https://github.com/pondsnails/SDTrainer/assets/46395085/256fed7e-5083-4171-a195-8f1fb2c2d82b)

![image](https://github.com/pondsnails/SDTrainer/assets/46395085/b965c537-ed32-4fa4-9e58-db5829157a65)
![image](https://github.com/pondsnails/SDTrainer/assets/46395085/3ca7d4e9-fb8e-4be2-8656-d9724d2b7458)
![image](https://github.com/pondsnails/SDTrainer/assets/46395085/b58a95c9-89bf-4b02-b12f-e8f93e5838a5)



## 3\.2. 考察 
想定よりも損失の減少が微量であったために、なぜSDがここまで効力を発揮しなかったのかを考察した結果、SDの計算方法に問題があることが分かった。本来、予測単語と正解単語との誤差が大きい場合、即ち大きく外れているときこそ、学習率を大きくするべきであるので、逆の方法を用いるべきだとわかった。
## 3\.3. SD2について
上記のことより、SDを変更したSD2を考案すべきだと考えた。よってSD2の定式化は以下の通り：

SD2 :A''=A'×∛Γ

記号は先ほどのものと同様とする。
## 3\.4. 性能評価②
同様の条件でSD2を用いてSD無、SD、SD2の比較評価を行う。つまり九つのモデルを比較することになる。
# 4\. 結果②と今後の発展と展開
## 4\.1. 結果
今回SD2を用いたモデルはSD無、SDのモデルと比べて明らかに見て取れる損失の減少をもたらした。しかしながら過学習が行われていることが評価用データセットの損失からも見て取れる。

![image](https://github.com/pondsnails/SDTrainer/assets/46395085/a0bc2258-b255-4648-9c86-de2d4e695059)
![image](https://github.com/pondsnails/SDTrainer/assets/46395085/f86745a5-9749-4138-9406-3994a2a2aedf)
![image](https://github.com/pondsnails/SDTrainer/assets/46395085/90decf78-ff38-4677-853a-9cdcb3ccacea)


## 4\.2. 今後の発展と展開
SDとSD2の導入が言語モデルの性能向上に寄与する可能性を示唆したものの、SD2の結果よりデータセットの少なさ故に、過学習が行われている可能性がある。今後の展望としては、SD、SD2が実用的になるには、今後の研究で計算方法の改善、新たな評価指標の導入、データセットの大規模化に焦点を当て、検証が必要であることが分かった。

# 参考文献：
- Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,

Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, 2017.

- 木藤冬樹 . コノテーション―感・主観のはざまで― . 東京外国語大学論集. 1991 , 第43号
2

